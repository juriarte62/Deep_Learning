{"cells":[{"cell_type":"markdown","id":"76354768","metadata":{"id":"76354768"},"source":["**NOTE: See the Assignments page in the \"Start here\" module for the rubric that will be used to grade labs.**"]},{"cell_type":"code","execution_count":1,"id":"b5a953ff","metadata":{"id":"b5a953ff","executionInfo":{"status":"ok","timestamp":1743000168074,"user_tz":240,"elapsed":25798,"user":{"displayName":"Johnny Uriarte","userId":"07630539884879584070"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense"]},{"cell_type":"markdown","id":"396affc0","metadata":{"id":"396affc0"},"source":["For this lab we will use the Scikit-Learn [breast cancer](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset) toy dataset. It has various attributes of cell nuclei as features."]},{"cell_type":"code","execution_count":2,"id":"4e280335","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4e280335","executionInfo":{"status":"ok","timestamp":1743000194726,"user_tz":240,"elapsed":41,"user":{"displayName":"Johnny Uriarte","userId":"07630539884879584070"}},"outputId":"b443d61c-ee3a-449f-ea47-e4c8c4bc316c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Samples: 569\tFeatures: 30\n"]}],"source":["(X, y) = load_breast_cancer(return_X_y=True)\n","m = X.shape[0]\n","n_features = X.shape[1]\n","print(\"Samples: %d\\tFeatures: %d\"%(m,n_features))"]},{"cell_type":"markdown","id":"dc004838","metadata":{"id":"dc004838"},"source":["**1. Use the Scikit-Learn `train_test_split()` function to split X and y into train and test sets, with 80% of the data as the training set, and store in `X_train`, `X_test`, `y_train`, `y_test`.**\n","  - Hint: check out the examples in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n","  - Hint: look at the arguments `test_size` and `train_size`."]},{"cell_type":"code","execution_count":3,"id":"ecb691a3","metadata":{"id":"ecb691a3","executionInfo":{"status":"ok","timestamp":1743000252848,"user_tz":240,"elapsed":15,"user":{"displayName":"Johnny Uriarte","userId":"07630539884879584070"}}},"outputs":[],"source":["# YOUR CODE HERE\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","id":"e4005442","metadata":{"id":"e4005442"},"source":["**2. Fit a [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to just the training input. Use the fit model to scale both the training and testing inputs, storing the scaled data back in `X_train` and `X_test` again.**"]},{"cell_type":"code","execution_count":4,"id":"faa839c2","metadata":{"id":"faa839c2","executionInfo":{"status":"ok","timestamp":1743000311234,"user_tz":240,"elapsed":40,"user":{"displayName":"Johnny Uriarte","userId":"07630539884879584070"}}},"outputs":[],"source":["# YOUR CODE HERE\n","scaler = MinMaxScaler()\n","scaler.fit(X_train)\n","X_train = scaler.transform(X_train)\n","X_test = scaler.transform(X_test)"]},{"cell_type":"markdown","id":"d61aed64","metadata":{"id":"d61aed64"},"source":["**3. Define a Keras sequential model with two hidden layers, the first with 64 neurons, the second with 32 neurons, and a single output neuron. Use ReLU activation for the two hidden layers and sigmoid for the output layer**\n","- Hint: remember that the input shape must match the number of features we have.\n","- Hint: choose appropriate activation functions for each layer"]},{"cell_type":"code","execution_count":6,"id":"28acb89d","metadata":{"id":"28acb89d","executionInfo":{"status":"ok","timestamp":1743000904153,"user_tz":240,"elapsed":73,"user":{"displayName":"Johnny Uriarte","userId":"07630539884879584070"}}},"outputs":[],"source":["# YOUR CODE HERE\n","model = Sequential()\n","model.add(Dense(64, activation='relu', input_shape=(n_features,))) # layer 1: 64 neurons, relu activation, input size will match number of features (n_features)\n","model.add(Dense(32, activation='relu')) # layer 2: 32 neurons, relu activation\n","model.add(Dense(1, activation='sigmoid')) # layer 3: 1 neuron, sigmoid activation"]},{"cell_type":"markdown","id":"704a3149","metadata":{"id":"704a3149"},"source":["**4. Compile the model with binary cross-entropy loss, accuracy for metrics, and a Stochastic Gradient Descent optimizer.**"]},{"cell_type":"code","execution_count":7,"id":"f81f3d59","metadata":{"id":"f81f3d59","executionInfo":{"status":"ok","timestamp":1743001079673,"user_tz":240,"elapsed":40,"user":{"displayName":"Johnny Uriarte","userId":"07630539884879584070"}}},"outputs":[],"source":["# YOUR CODE HERE\n","model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n","# sgd = Stochastic Gradient Descent"]},{"cell_type":"markdown","id":"3e9d9681","metadata":{"id":"3e9d9681"},"source":["**5. Fit the model to the training data, using 16 epochs and a batch size of 8.**"]},{"cell_type":"code","execution_count":8,"id":"11d37ada","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"11d37ada","executionInfo":{"status":"ok","timestamp":1743001175352,"user_tz":240,"elapsed":3628,"user":{"displayName":"Johnny Uriarte","userId":"07630539884879584070"}},"outputId":"ede2f0fd-394d-4990-d574-5a3307343603"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8134 - loss: 0.6738\n","Epoch 2/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8000 - loss: 0.6543\n","Epoch 3/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8065 - loss: 0.6287\n","Epoch 4/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8029 - loss: 0.6045\n","Epoch 5/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8471 - loss: 0.5641\n","Epoch 6/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8985 - loss: 0.5361\n","Epoch 7/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9002 - loss: 0.4982\n","Epoch 8/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9334 - loss: 0.4606\n","Epoch 9/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9044 - loss: 0.4353\n","Epoch 10/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9207 - loss: 0.3912\n","Epoch 11/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9361 - loss: 0.3611\n","Epoch 12/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9319 - loss: 0.3438\n","Epoch 13/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9400 - loss: 0.2996\n","Epoch 14/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9280 - loss: 0.2883\n","Epoch 15/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9576 - loss: 0.2571\n","Epoch 16/16\n","\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9367 - loss: 0.2597\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7976c783e190>"]},"metadata":{},"execution_count":8}],"source":["# YOUR CODE HERE\n","model.fit(X_train, y_train, epochs=16, batch_size=8)"]},{"cell_type":"markdown","id":"de82e711","metadata":{"id":"de82e711"},"source":["## Torch (optional tutorial)"]},{"cell_type":"code","execution_count":9,"id":"b3d31844","metadata":{"id":"b3d31844","executionInfo":{"status":"ok","timestamp":1743001186532,"user_tz":240,"elapsed":6762,"user":{"displayName":"Johnny Uriarte","userId":"07630539884879584070"}}},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"markdown","id":"0bb49f5a","metadata":{"id":"0bb49f5a"},"source":["Create train/test split and batches"]},{"cell_type":"code","execution_count":10,"id":"cd477ae4","metadata":{"id":"cd477ae4","executionInfo":{"status":"ok","timestamp":1743001186599,"user_tz":240,"elapsed":68,"user":{"displayName":"Johnny Uriarte","userId":"07630539884879584070"}}},"outputs":[],"source":["class dataset(Dataset):\n","  def __init__(self,x,y):\n","    self.x = torch.tensor(x,dtype=torch.float32)\n","    self.y = torch.tensor(y,dtype=torch.float32)\n","    self.length = self.x.shape[0]\n","\n","  def __getitem__(self,idx):\n","    return self.x[idx],self.y[idx]\n","  def __len__(self):\n","    return self.length\n","\n","ds_train = dataset(X_train, y_train)\n","dl_train = DataLoader(ds_train, batch_size=16, shuffle=True)\n","\n","ds_test = dataset(X_test, y_test)\n","dl_test = DataLoader(ds_test, batch_size=16, shuffle=True)"]},{"cell_type":"markdown","id":"fd58397d","metadata":{"id":"fd58397d"},"source":["### Define the model\n","\n","In the `Week1Network` constructor, we'll create `self.layer1` and `self.layer2` as linear layers. In `forward()`, we'll pass values from layer1 to this layer2, then to a ReLU activation, and store in `x`. Note that when defining a layer, `in_features` must match the `out_features` of the previous layer."]},{"cell_type":"code","execution_count":11,"id":"61883c5d","metadata":{"id":"61883c5d","executionInfo":{"status":"ok","timestamp":1743001196080,"user_tz":240,"elapsed":39,"user":{"displayName":"Johnny Uriarte","userId":"07630539884879584070"}}},"outputs":[],"source":["class Week1Network(torch.nn.Module):\n","    def __init__(self, n_features):\n","        super(Week1Network, self).__init__()\n","        self.layer1 = torch.nn.Linear(n_features, 64)\n","        self.layer2 = torch.nn.Linear(64, 32)\n","        self.layer_out = torch.nn.Linear(32,1)\n","\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        x = torch.relu(x)\n","        x = self.layer2(x)\n","        x = torch.relu(x)\n","        x = self.layer_out(x)\n","        x = torch.sigmoid(x)\n","\n","        return x"]},{"cell_type":"markdown","id":"52c7bd96","metadata":{"id":"52c7bd96"},"source":["### Train the model"]},{"cell_type":"markdown","id":"68f77cc9","metadata":{"id":"68f77cc9"},"source":["We'll create an instance of our `Week1Network` class, and store in `model`, then create a Torch SGD optimizer object with a learning rate of 0.01, and store in `opt`. Finally, we'll create a Torch loss function object of the appropriate type for our binary classification problem, and store in `loss_fn`. Note that the initializer takes the number of features as a parameter. This must match the number of features in the breast cancer dataset that is loaded above."]},{"cell_type":"code","execution_count":12,"id":"496b59f3","metadata":{"id":"496b59f3","executionInfo":{"status":"ok","timestamp":1743001204479,"user_tz":240,"elapsed":6074,"user":{"displayName":"Johnny Uriarte","userId":"07630539884879584070"}}},"outputs":[],"source":["model = Week1Network(n_features)\n","opt = torch.optim.SGD(model.parameters(),lr=0.1)\n","loss_fn = torch.nn.BCELoss()"]},{"cell_type":"code","execution_count":13,"id":"643f47ed","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"643f47ed","executionInfo":{"status":"ok","timestamp":1743001205022,"user_tz":240,"elapsed":544,"user":{"displayName":"Johnny Uriarte","userId":"07630539884879584070"}},"outputId":"a8a2a4e2-a630-4c2c-d6cd-5a7aefa13855"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:  1 loss: 0.629791\n","epoch:  2 loss: 0.596878\n","epoch:  3 loss: 0.519496\n","epoch:  4 loss: 0.401336\n","epoch:  5 loss: 0.174306\n","epoch:  6 loss: 0.098769\n","epoch:  7 loss: 0.084555\n","epoch:  8 loss: 0.643045\n","epoch:  9 loss: 0.030925\n","epoch: 10 loss: 0.168089\n","epoch: 11 loss: 0.341667\n","epoch: 12 loss: 0.038016\n","epoch: 13 loss: 0.066692\n","epoch: 14 loss: 0.038799\n","epoch: 15 loss: 0.026909\n","epoch: 16 loss: 0.006782\n"]}],"source":["size = len(dl_train.dataset)\n","\n","for i in range(16): # epochs\n","    for batch, (X_batch, y_batch) in enumerate(dl_train):\n","        pred = model(X_batch)\n","        loss = loss_fn(pred, y_batch.reshape(-1,1))\n","\n","        # Backpropagation\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","\n","    print(f\"epoch: {i+1:2d} loss: {loss:>7f}\")\n"]},{"cell_type":"markdown","id":"b8fb2f54","metadata":{"id":"b8fb2f54"},"source":["### Testing"]},{"cell_type":"code","execution_count":14,"id":"8281fc87","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8281fc87","executionInfo":{"status":"ok","timestamp":1743001236834,"user_tz":240,"elapsed":12,"user":{"displayName":"Johnny Uriarte","userId":"07630539884879584070"}},"outputId":"eaea36de-cd78-417a-f9fe-b05b7b366a68"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test Error: \n"," Accuracy: 97.4%, Avg loss: 0.063842 \n","\n"]}],"source":["num_batches = len(dl_test)\n","test_loss, correct = 0, 0\n","\n","with torch.no_grad():\n","    for X_batch, y_batch in dl_test:\n","        pred = model(X_batch)\n","        test_loss += loss_fn(pred, y_batch.reshape(-1,1))\n","        correct += (pred.reshape(-1).round() == y_batch).sum().item()\n","\n","test_loss /= num_batches\n","correct /= len(dl_test.dataset)\n","print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"]},{"cell_type":"code","execution_count":null,"id":"98929f17","metadata":{"id":"98929f17"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}